{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOUNUgr3pbwz2gAY6teH3Sj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvHm-Cd6ale7",
        "colab_type": "text"
      },
      "source": [
        "#Cost Function\n",
        "Let's first define a few variables that we will need to use:\n",
        "\n",
        "- L = total number of layers in the network\n",
        "\n",
        "- $s_l$= number of units (not counting bias unit) in layer l\n",
        "\n",
        "- K = number of output units/classes\n",
        "\n",
        "For binary classification, y = 0 or 1, which has 1 output unit;\n",
        "\n",
        "\n",
        "For multi-class (*K* class) classification, y $\\in ℝ^K$ (i.e., $h_\\Theta(x)\\in ℝ^K$, and $h_\\Theta(x)_i = i^{th}$output), which has *K* output units;\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In **Neural Network**, cost function is\n",
        "\n",
        "\\begin{equation*} J(\\Theta) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[y^{(i)}_k \\log ((h_\\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\\log (1 - (h_\\Theta(x^{(i)}))_k)\\right] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} ( \\Theta_{j,i}^{(l)})^2\\end{equation*}\n",
        "\n",
        "Note:\n",
        "- The double sum adds up the logistic regression costs calculated for each cell in the output layer;\n",
        "- The triple sum adds up for all individual $\\Theta$\n",
        "- the i in the triple sum does **not** refer to training example i \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEXOYanuesqn",
        "colab_type": "text"
      },
      "source": [
        "#Backpropogation Algorithm\n",
        "\"Backpropagation\" is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression.\n",
        "\n",
        "Need to compute:\n",
        "- $\\min_\\Theta J(\\Theta)$\n",
        "- $\\dfrac{\\partial}{\\partial \\Theta_{i,j}^{(l)}}J(\\Theta)$\n",
        "\n",
        "**Backpropogation Algorithm:**\n",
        "\n",
        "Given training set {($x^{(1)}, y^{(1)}$),...,($x^{(m)}, y^{(m)}$)}\n",
        "\n",
        "1. Set $\\Delta_{ij}^{(l)} = 0$ (for all $l, i, j$ )\n",
        "\n",
        "2. For *i* = 1 to m\n",
        "    set $a^{(1)} = x^{(i)}$\n",
        "    perform **forward propogation** to compute $a^{(l)}$ for $l$ = 2,3,...,L\n",
        "    \n",
        "    ![alt text](https://raw.githubusercontent.com/wongchihaul/Coursera-ML/master/Pics/forward%20propogation.png)\n",
        "    \n",
        "3. Using $y^{(i)}$, compute $\\delta^{(L)} = a^{(L)} - y^{(i)}$\n",
        "    \n",
        "    Where L is our total number of layers and $a^{(L)}$is the vector of outputs of the activation units for the last layer. So our \"error values\" for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y. \n",
        "\n",
        "4. Compute $\\delta^{(L-1)}, \\delta^{(L-2)}, ..., \\delta^{(2)}$ by using \\begin{equation*}\\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .*\\ a^{(l)}\\ .*\\ (1 - a^{(l)})\\end{equation*}\n",
        "    \n",
        "    [Remember there is no $\\delta^{(1)}$]\n",
        "\n",
        "    The g-prime derivative terms can also be written out as:\\begin{equation}g^{'}(z^{(l)}) = a^{(l)}\\ .*\\ (1 - a^{(l)})\\end{equation}\n",
        "5.$\\Delta_{ij}^{(l)} := \\Delta_{ij}^{(l)} + a^{(l)}_j\\delta^{(l+1)}_i$ and its vectorization format is \\begin{equation}\\Delta^{(l)} :=  \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T\\end{equation}\n",
        "\n",
        "    Hence we update our new $\\Delta$ matrix\n",
        "\n",
        "- $D^{(l)}_{ij} := \\dfrac{1}{m}\\left(\\Delta^{(l)}_{ij} + \\lambda\\Theta^{(l)}_{i,j}\\right)$, if $j \\not=0$\n",
        "- $D^{(l)}_{ij} := \\dfrac{1}{m}\\Delta^{(l)}_{ij}$, if$j =0$\n",
        "\n",
        "    ![alt text](https://raw.githubusercontent.com/wongchihaul/Coursera-ML/master/Pics/backpropogation.png)\n",
        "\n",
        "---\n",
        "\n",
        "The capital-delta matrix D is used as an \"accumulator\" to add up our values as we go along and eventually compute our partial derivative. Thus we get \\begin{equation}\\dfrac{\\partial}{\\partial \\Theta_{i,j}^{(l)}}J(\\Theta) = D^{(l)}_{(ij)}\\end{equation}\n",
        "​\t"
      ]
    }
  ]
}